{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59cfc6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf3b74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcda03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read the data from the files\n",
    "check if you nead to update path\n",
    "'''\n",
    "EEG_features_3_sec = pd.read_csv(\"output_b/df_EEG_b_features_Tob_-3_toTob_0.csv\")\n",
    "EEG_features_2_sec = pd.read_csv(\"output_b/df_EEG_b_features_Tob_-2_toTob_0.csv\")\n",
    "EEG_features_3_until_2_sec = pd.read_csv(\"output_b/df_EEG_b_features_Tob_-3_toTob_-2.csv\")\n",
    "paymentMethod = pd.read_csv(\"df_painOfPayment_method.csv\")\n",
    "painOfPayment_score = pd.read_csv(\"df_painOfPayment_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a38ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all(EEG_features):\n",
    "    df_merged = pd.merge(EEG_features, paymentMethod, on=\"subject\", how=\"inner\")\n",
    "    df_merged = pd.merge(df_merged, painOfPayment_score, on=\"subject\", how=\"inner\")\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e1856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "initialize variables\n",
    "'''\n",
    "waves = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "df_data_3_sec = merge_all(EEG_features_3_sec)\n",
    "df_data_2_sec = merge_all(EEG_features_2_sec)\n",
    "df_data_3_until_2_sec = merge_all(EEG_features_3_until_2_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35330cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RandomForest_hyperparameters(X_train,y_train, classification = True):\n",
    "    \n",
    "    # Number of trees in Random Forest\n",
    "    n_estimators = [50, 100, 200, 300]\n",
    "    \n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['sqrt', 'log2', None]\n",
    "    \n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(5, 50, 10)]\n",
    "    # Add the default as a possible value\n",
    "    max_depth.append(None)\n",
    "    \n",
    "    # min_samples_split\n",
    "    min_split = [2,3,4,5]\n",
    "      \n",
    "    param_grid = {'n_estimators': n_estimators,\n",
    "                  'max_features': max_features,\n",
    "                  'max_depth':max_depth,\n",
    "                  'min_samples_split': min_split\n",
    "                 }\n",
    "    \n",
    "    if classification:\n",
    "        rf_base = RandomForestClassifier(random_state = 42)\n",
    "    else:\n",
    "        rf_base = RandomForestRegressor(random_state = 42)\n",
    "    \n",
    "    '''\n",
    "    Create the grid search Random Forest:\n",
    "    GridSearchCV\n",
    "        GridSearch: taking all of paramters combination from the grid parameters to find the best hyper parameters\n",
    "        CV: using cross validation \n",
    "    cv = 4 (4-fold cross validation) --> validation_set = 20% from all data (0.8 X 0.25 = 0.2)\n",
    "    cv = LeaveOneOut()\n",
    "    ''' \n",
    "    # note: if regression and cv = LeaveOneOut() --> use scoring = 'neg_mean_absolute_error'\n",
    "    rf_search = GridSearchCV(estimator = rf_base, param_grid = param_grid, \n",
    "                               cv = LeaveOneOut(), verbose = 2, n_jobs = -1, scoring = 'neg_mean_absolute_error')\n",
    "    \n",
    "    # Fit the random search model\n",
    "    rf_search.fit(X_train, y_train)\n",
    "    \n",
    "    return rf_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5441855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SVC_hyperparameters(X_train, y_train):\n",
    "    \n",
    "    # \n",
    "    C = [0.1,1, 10, 100, 1000]\n",
    "    \n",
    "    # \n",
    "    gamma = [1,0.1,0.01,0.001,0.0001]\n",
    "       \n",
    "    param_grid = {'C': C, 'gamma': gamma}\n",
    "    \n",
    "    SVC_base = SVC(random_state = 42)\n",
    "    \n",
    "    '''\n",
    "    Create the grid search SVC:\n",
    "    GridSearchCV\n",
    "        GridSearch: taking all of paramters combination from the grid parameters to find the best hyper parameters\n",
    "        CV: using cross validation \n",
    "    cv = 4 (4-fold cross validation) --> validation_set = 20% from all data (0.8 X 0.25 = 0.2)\n",
    "    cv = LeaveOneOut()\n",
    "    ''' \n",
    "    \n",
    "    SVC_search = GridSearchCV(estimator = SVC_base, param_grid = param_grid, \n",
    "                               cv = LeaveOneOut(), verbose = 2, n_jobs = -1)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    SVC_search.fit(X_train, y_train)\n",
    "    \n",
    "    return SVC_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5597bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get model hyperParameters\n",
    "'''\n",
    "def get_classification_hyperParameters(X_train, y_train):\n",
    "    rfc_serach = get_RandomForest_hyperparameters(X_train, y_train)\n",
    "    print(rfc_serach.best_params_)\n",
    "\n",
    "    SVC_serach = get_SVC_hyperparameters(X_train, y_train)\n",
    "    print(SVC_serach.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b66bd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get model hyperParameters\n",
    "'''\n",
    "def get_regression_hyperParameters(X_train, y_train):\n",
    "    rfc_serach = get_RandomForest_hyperparameters(X_train, y_train, classification = False)\n",
    "    print(rfc_serach.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe82c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_classification_model(X_train, X_test, y_train, y_test, n_estimators, max_features , max_depth,  min_samples_split):\n",
    "    print(\"################# RandomForest #################\")\n",
    "   \n",
    "    rfc = RandomForestClassifier(n_estimators = n_estimators, max_features = max_features, max_depth = max_depth,  min_samples_split = min_samples_split, random_state = 42)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    print_classification_matrics(rfc, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af4a7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_regression_model(X_train, X_test, y_train, y_test, n_estimators, max_features , max_depth,  min_samples_split):\n",
    "    print(\"################# RandomForest #################\")\n",
    "    \n",
    "    rfc = RandomForestRegressor(n_estimators = n_estimators, max_features = max_features, max_depth = max_depth,  min_samples_split = min_samples_split, random_state = 42)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    print_regression_matrics(rfc, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad59e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC_model(X_train, X_test, y_train, y_test, C = 0.1, gamma = 1):\n",
    "    print(\"################# SVC #################\")\n",
    "    \n",
    "    svc = SVC(C = C, gamma = gamma, random_state = 42)\n",
    "    svc.fit(X_train, y_train) \n",
    "    print_classification_matrics(svc, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d9f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calssification_rule_based_model(X_train, X_test, y_train, y_test):\n",
    "        \n",
    "    print (\"################# DummyClassifier - most_frequent #################\")\n",
    "    dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    print_classification_matrics(dummy_clf, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # \"stratified\" strategy - generates predictions by randomly selecting labels according to the distribution of categories in the training data.\n",
    "    print (\"################# DummyClassifier - stratified #################\")\n",
    "    dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    print_classification_matrics(dummy_clf, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    print(\"################# LogisticRegression #################\")\n",
    "    Logistic_clf = LogisticRegression(random_state=42)\n",
    "    Logistic_clf.fit(X_train, y_train)\n",
    "    print_classification_matrics(Logistic_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8f51280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_rule_based_model(X_train, X_test, y_train, y_test):\n",
    "        \n",
    "    print (\"################# DummyClassifier - median #################\")\n",
    "    dummy_clf = DummyRegressor(strategy=\"median\")\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    print_regression_matrics(dummy_clf, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    print (\"################# DummyClassifier - mean #################\")\n",
    "    dummy_clf = DummyRegressor(strategy=\"mean\")\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    print_regression_matrics(dummy_clf, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    print(\"################# LinearRegression #################\")\n",
    "    Linear_clf = LinearRegression()\n",
    "    Linear_clf.fit(X_train, y_train)\n",
    "    print_regression_matrics(Linear_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3676c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_matrics(model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(\"####### train data results #######\")\n",
    "    print_classification_matrics_helper(model, X_train, y_train)\n",
    "    \n",
    "    print(\"####### test data results #######\")\n",
    "    print_classification_matrics_helper(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a53e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_matrics_helper(model, X, y):\n",
    "    \n",
    "    predictions = model.predict(X)\n",
    "    curr_confusion_matrix = confusion_matrix(y, predictions)\n",
    "    print(\"confusion_matrix: \")\n",
    "    print(curr_confusion_matrix)\n",
    "    curr_classification_report = classification_report(y ,predictions, zero_division=0)\n",
    "    print(\"classification_report: \")\n",
    "    print(curr_classification_report)\n",
    "    print(\"Accuracy: \", model.score(X, y) , \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dd2674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_regression_matrics(model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(\"####### train data results #######\")\n",
    "    print_regression_matrics_helper(model, X_train, y_train)\n",
    "    \n",
    "    print(\"####### test data results #######\")\n",
    "    print_regression_matrics_helper(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d08196f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_regression_matrics_helper(model, X, y):\n",
    "    \n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    plt.scatter(y, predictions, alpha = 0.6)\n",
    "    plt.xlabel('True score', fontsize=12)\n",
    "    plt.ylabel('Predicted score')\n",
    "    plt.xlim(3, 29)\n",
    "    plt.ylim(3, 29)\n",
    "    plt.show()\n",
    "    \n",
    "    sns.histplot((y-predictions), bins=50, kde=True)\n",
    "    plt.title(\"distibution of True_Score - Predicted_Score \", fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    print('MAE:', metrics.mean_absolute_error(y, predictions))\n",
    "    print('MSE:', metrics.mean_squared_error(y, predictions))\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(y, predictions)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ab5a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get model predictions\n",
    "Call predict on the estimator with the best found parameters.\n",
    "'''\n",
    "def get_model_1_predicrions(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # baseline\n",
    "    # Rule-based models\n",
    "    dummy_clf = calssification_rule_based_model(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # random forest \n",
    "    # using: {'n_estimators': 200, 'max_features': 'sqrt', 'max_depth': 5, 'min_samples_split': 5}\n",
    "    rfc = RandomForest_classification_model(X_train, X_test, y_train, y_test, n_estimators = 200, max_features = 'sqrt', max_depth = 5,  min_samples_split = 5)\n",
    "\n",
    "    # SVM\n",
    "    # using: {'C': 0.1, 'gamma': 1}\n",
    "    #svc = SVC_model(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ded11526",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get model predictions\n",
    "Call predict on the estimator with the best found parameters.\n",
    "'''\n",
    "def get_model_2_predicrions(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # baseline\n",
    "    # Rule-based models\n",
    "    dummy_clf = calssification_rule_based_model(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # random forest \n",
    "    # using: {'n_estimators': 200, 'max_features': 'sqrt', 'max_depth': 5, 'min_samples_split': 4}\n",
    "    rfc = RandomForest_classification_model(X_train, X_test, y_train, y_test, n_estimators = 200, max_features = 'sqrt', max_depth = 5,  min_samples_split = 4)\n",
    "\n",
    "    # SVM\n",
    "    # using: {'C': 0.1, 'gamma': 1}\n",
    "    # svc = SVC_model(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bd8ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get model predictions\n",
    "Call predict on the estimator with the best found parameters.\n",
    "'''\n",
    "def get_model_3_predicrions(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # baseline\n",
    "    # Rule-based models\n",
    "    dummy_clf = regression_rule_based_model(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # random forest \n",
    "    # using: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 100}\n",
    "    rfc = RandomForest_regression_model(X_train, X_test, y_train, y_test, n_estimators = 50, max_features = 'sqrt', max_depth = 5,  min_samples_split = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "106c9114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# DummyClassifier - most_frequent #################\n",
      "####### train data results #######\n",
      "confusion_matrix: \n",
      "[[ 0 18  0]\n",
      " [ 0 29  0]\n",
      " [ 0 14  0]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cash       0.00      0.00      0.00        18\n",
      " Credit Card       0.48      1.00      0.64        29\n",
      "  Smartphone       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.48        61\n",
      "   macro avg       0.16      0.33      0.21        61\n",
      "weighted avg       0.23      0.48      0.31        61\n",
      "\n",
      "Accuracy:  0.47540983606557374 \n",
      "\n",
      "####### test data results #######\n",
      "confusion_matrix: \n",
      "[[0 5 0]\n",
      " [0 3 0]\n",
      " [0 8 0]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cash       0.00      0.00      0.00         5\n",
      " Credit Card       0.19      1.00      0.32         3\n",
      "  Smartphone       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.19        16\n",
      "   macro avg       0.06      0.33      0.11        16\n",
      "weighted avg       0.04      0.19      0.06        16\n",
      "\n",
      "Accuracy:  0.1875 \n",
      "\n",
      "################# DummyClassifier - stratified #################\n",
      "####### train data results #######\n",
      "confusion_matrix: \n",
      "[[ 7  8  3]\n",
      " [ 2 20  7]\n",
      " [10  3  1]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cash       0.37      0.39      0.38        18\n",
      " Credit Card       0.65      0.69      0.67        29\n",
      "  Smartphone       0.09      0.07      0.08        14\n",
      "\n",
      "    accuracy                           0.46        61\n",
      "   macro avg       0.37      0.38      0.38        61\n",
      "weighted avg       0.44      0.46      0.45        61\n",
      "\n",
      "Accuracy:  0.45901639344262296 \n",
      "\n",
      "####### test data results #######\n",
      "confusion_matrix: \n",
      "[[2 1 2]\n",
      " [0 3 0]\n",
      " [2 5 1]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cash       0.50      0.40      0.44         5\n",
      " Credit Card       0.33      1.00      0.50         3\n",
      "  Smartphone       0.33      0.12      0.18         8\n",
      "\n",
      "    accuracy                           0.38        16\n",
      "   macro avg       0.39      0.51      0.38        16\n",
      "weighted avg       0.39      0.38      0.32        16\n",
      "\n",
      "Accuracy:  0.375 \n",
      "\n",
      "################# LogisticRegression #################\n",
      "####### train data results #######\n",
      "confusion_matrix: \n",
      "[[ 0 18  0]\n",
      " [ 0 29  0]\n",
      " [ 0 14  0]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cash       0.00      0.00      0.00        18\n",
      " Credit Card       0.48      1.00      0.64        29\n",
      "  Smartphone       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.48        61\n",
      "   macro avg       0.16      0.33      0.21        61\n",
      "weighted avg       0.23      0.48      0.31        61\n",
      "\n",
      "Accuracy:  0.47540983606557374 \n",
      "\n",
      "####### test data results #######\n",
      "confusion_matrix: \n",
      "[[0 5 0]\n",
      " [0 3 0]\n",
      " [0 8 0]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cash       0.00      0.00      0.00         5\n",
      " Credit Card       0.19      1.00      0.32         3\n",
      "  Smartphone       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.19        16\n",
      "   macro avg       0.06      0.33      0.11        16\n",
      "weighted avg       0.04      0.19      0.06        16\n",
      "\n",
      "Accuracy:  0.1875 \n",
      "\n",
      "################# RandomForest #################\n",
      "####### train data results #######\n",
      "confusion_matrix: \n",
      "[[18  0  0]\n",
      " [ 0 29  0]\n",
      " [ 0  3 11]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cash       1.00      1.00      1.00        18\n",
      " Credit Card       0.91      1.00      0.95        29\n",
      "  Smartphone       1.00      0.79      0.88        14\n",
      "\n",
      "    accuracy                           0.95        61\n",
      "   macro avg       0.97      0.93      0.94        61\n",
      "weighted avg       0.96      0.95      0.95        61\n",
      "\n",
      "Accuracy:  0.9508196721311475 \n",
      "\n",
      "####### test data results #######\n",
      "confusion_matrix: \n",
      "[[2 3 0]\n",
      " [1 2 0]\n",
      " [0 6 2]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cash       0.67      0.40      0.50         5\n",
      " Credit Card       0.18      0.67      0.29         3\n",
      "  Smartphone       1.00      0.25      0.40         8\n",
      "\n",
      "    accuracy                           0.38        16\n",
      "   macro avg       0.62      0.44      0.40        16\n",
      "weighted avg       0.74      0.38      0.41        16\n",
      "\n",
      "Accuracy:  0.375 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MAIN function to get model 1\n",
    "'''\n",
    "# Model 1 (first question): Tries to predict the payment method based on the EEG_features and painOfPayment\n",
    "\n",
    "X = df_data_3_sec[waves]\n",
    "y = df_data_3_sec['Payment_method']\n",
    "\n",
    "# test size is 20% from all data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# get_classification_hyperParameters(X_train, y_train)\n",
    "# random forest (3 seconds before ToB):\n",
    "# cv = 4: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 200}\n",
    "# cv = 5: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 100}\n",
    "# cv = LeaveOneOut(): {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 200}\n",
    "\n",
    "# random forest - stratify (3 seconds before ToB):\n",
    "# cv = 4: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 3, 'n_estimators': 200}\n",
    "# cv = 5: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 3, 'n_estimators': 50}\n",
    "# cv = LeaveOneOut(): {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 50}\n",
    "\n",
    "# SVC:\n",
    "# cv = 4: {'C': 100, 'gamma': 1}\n",
    "# cv = 5: {'C': 0.1, 'gamma': 1}\n",
    "# cv = LeaveOneOut() : {'C': 0.1, 'gamma': 1}\n",
    "\n",
    "get_model_1_predicrions(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da0b5509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# DummyClassifier - most_frequent #################\n",
      "####### train data results #######\n",
      "confusion_matrix: \n",
      "[[ 0  0 19]\n",
      " [ 0  0  7]\n",
      " [ 0  0 35]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.00      0.00      0.00        19\n",
      "         low       0.00      0.00      0.00         7\n",
      "      medium       0.57      1.00      0.73        35\n",
      "\n",
      "    accuracy                           0.57        61\n",
      "   macro avg       0.19      0.33      0.24        61\n",
      "weighted avg       0.33      0.57      0.42        61\n",
      "\n",
      "Accuracy:  0.5737704918032787 \n",
      "\n",
      "####### test data results #######\n",
      "confusion_matrix: \n",
      "[[0 0 8]\n",
      " [0 0 3]\n",
      " [0 0 5]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.00      0.00      0.00         8\n",
      "         low       0.00      0.00      0.00         3\n",
      "      medium       0.31      1.00      0.48         5\n",
      "\n",
      "    accuracy                           0.31        16\n",
      "   macro avg       0.10      0.33      0.16        16\n",
      "weighted avg       0.10      0.31      0.15        16\n",
      "\n",
      "Accuracy:  0.3125 \n",
      "\n",
      "################# DummyClassifier - stratified #################\n",
      "####### train data results #######\n",
      "confusion_matrix: \n",
      "[[ 8  0 11]\n",
      " [ 1  0  6]\n",
      " [10  8 17]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.42      0.42      0.42        19\n",
      "         low       0.00      0.00      0.00         7\n",
      "      medium       0.50      0.49      0.49        35\n",
      "\n",
      "    accuracy                           0.41        61\n",
      "   macro avg       0.31      0.30      0.30        61\n",
      "weighted avg       0.42      0.41      0.41        61\n",
      "\n",
      "Accuracy:  0.4098360655737705 \n",
      "\n",
      "####### test data results #######\n",
      "confusion_matrix: \n",
      "[[2 1 5]\n",
      " [0 1 2]\n",
      " [2 0 3]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.50      0.25      0.33         8\n",
      "         low       0.50      0.33      0.40         3\n",
      "      medium       0.30      0.60      0.40         5\n",
      "\n",
      "    accuracy                           0.38        16\n",
      "   macro avg       0.43      0.39      0.38        16\n",
      "weighted avg       0.44      0.38      0.37        16\n",
      "\n",
      "Accuracy:  0.375 \n",
      "\n",
      "################# LogisticRegression #################\n",
      "####### train data results #######\n",
      "confusion_matrix: \n",
      "[[ 0  0 19]\n",
      " [ 0  0  7]\n",
      " [ 0  0 35]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.00      0.00      0.00        19\n",
      "         low       0.00      0.00      0.00         7\n",
      "      medium       0.57      1.00      0.73        35\n",
      "\n",
      "    accuracy                           0.57        61\n",
      "   macro avg       0.19      0.33      0.24        61\n",
      "weighted avg       0.33      0.57      0.42        61\n",
      "\n",
      "Accuracy:  0.5737704918032787 \n",
      "\n",
      "####### test data results #######\n",
      "confusion_matrix: \n",
      "[[0 0 8]\n",
      " [0 0 3]\n",
      " [0 0 5]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.00      0.00      0.00         8\n",
      "         low       0.00      0.00      0.00         3\n",
      "      medium       0.31      1.00      0.48         5\n",
      "\n",
      "    accuracy                           0.31        16\n",
      "   macro avg       0.10      0.33      0.16        16\n",
      "weighted avg       0.10      0.31      0.15        16\n",
      "\n",
      "Accuracy:  0.3125 \n",
      "\n",
      "################# RandomForest #################\n",
      "####### train data results #######\n",
      "confusion_matrix: \n",
      "[[15  0  4]\n",
      " [ 1  3  3]\n",
      " [ 0  0 35]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.94      0.79      0.86        19\n",
      "         low       1.00      0.43      0.60         7\n",
      "      medium       0.83      1.00      0.91        35\n",
      "\n",
      "    accuracy                           0.87        61\n",
      "   macro avg       0.92      0.74      0.79        61\n",
      "weighted avg       0.88      0.87      0.86        61\n",
      "\n",
      "Accuracy:  0.8688524590163934 \n",
      "\n",
      "####### test data results #######\n",
      "confusion_matrix: \n",
      "[[0 0 8]\n",
      " [2 0 1]\n",
      " [1 0 4]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.00      0.00      0.00         8\n",
      "         low       0.00      0.00      0.00         3\n",
      "      medium       0.31      0.80      0.44         5\n",
      "\n",
      "    accuracy                           0.25        16\n",
      "   macro avg       0.10      0.27      0.15        16\n",
      "weighted avg       0.10      0.25      0.14        16\n",
      "\n",
      "Accuracy:  0.25 \n",
      "\n",
      "################# SVC #################\n",
      "####### train data results #######\n",
      "confusion_matrix: \n",
      "[[ 0  0 19]\n",
      " [ 0  0  7]\n",
      " [ 0  0 35]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.00      0.00      0.00        19\n",
      "         low       0.00      0.00      0.00         7\n",
      "      medium       0.57      1.00      0.73        35\n",
      "\n",
      "    accuracy                           0.57        61\n",
      "   macro avg       0.19      0.33      0.24        61\n",
      "weighted avg       0.33      0.57      0.42        61\n",
      "\n",
      "Accuracy:  0.5737704918032787 \n",
      "\n",
      "####### test data results #######\n",
      "confusion_matrix: \n",
      "[[0 0 8]\n",
      " [0 0 3]\n",
      " [0 0 5]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.00      0.00      0.00         8\n",
      "         low       0.00      0.00      0.00         3\n",
      "      medium       0.31      1.00      0.48         5\n",
      "\n",
      "    accuracy                           0.31        16\n",
      "   macro avg       0.10      0.33      0.16        16\n",
      "weighted avg       0.10      0.31      0.15        16\n",
      "\n",
      "Accuracy:  0.3125 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MAIN function to get model 2 - classification\n",
    "'''\n",
    "# Model 2 (second question) : Tries to predict the painOfPayment score based on the EEG_features\n",
    "\n",
    "X = df_data_2_sec[waves]\n",
    "y = df_data_2_sec['PainOfPayment_categorical_score']\n",
    "\n",
    "# test size is 20% from all data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# get_classification_hyperParameters(X_train, y_train)\n",
    "\n",
    "# random forest (3 seconds before ToB):\n",
    "# cv = 4: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 3, 'n_estimators': 200}\n",
    "# cv = 5: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 50}\n",
    "# cv = LeaveOneOut(): {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 50}\n",
    "\n",
    "# random forest (2 seconds before ToB):\n",
    "# cv = 4: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 3, 'n_estimators': 100}\n",
    "# cv = 5: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 50}\n",
    "# cv = LeaveOneOut(): {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n",
    "\n",
    "# SVC:\n",
    "# cv = 4: {'C': 0.1, 'gamma': 1}\n",
    "# cv = 5: {'C': 0.1, 'gamma': 1}\n",
    "# cv = LeaveOneOut() : {'C': 0.1, 'gamma': 1}\n",
    "\n",
    "get_model_2_predicrions(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211f74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 61 folds for each of 528 candidates, totalling 32208 fits\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MAIN function to get model 3 - regression\n",
    "'''\n",
    "# Model 3 (second question) : Tries to predict the painOfPayment score based on the EEG_features\n",
    "\n",
    "X = df_data_2_sec[waves]\n",
    "y = df_data_2_sec['PainOfPayment_score']\n",
    "\n",
    "# test size is 20% from all data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "get_regression_hyperParameters(X_train, y_train)\n",
    "\n",
    "# random forest (3 seconds before ToB):\n",
    "# with min_samples_split and cv = 4: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 100}\n",
    "# with min_samples_split and cv = 5: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 50}\n",
    "# with min_samples_split and cv = LeaveOneOut(): \n",
    "\n",
    "# random forest (2 seconds before ToB):\n",
    "# with min_samples_split and cv = 4: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n",
    "# with min_samples_split and cv = 5: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 50}\n",
    "# with min_samples_split and cv = LeaveOneOut():\n",
    "\n",
    "get_model_3_predicrions(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7239c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
